# SILAFAE 2024
Workshop para SILAFAE 2024 - Arturo Sanchez

## **Workshop Syllabus**

### **Workshop Title:**
**Practical Large-Scale Data Analysis and AI Tools in Scientific Research: Empowering Latin American Researchers**

### **Workshop Overview:**

This intensive 8-hour hands-on workshop, divided into four 2-hour sessions, is designed specifically for Latin American physics students and researchers.
The workshop aims to empower participants by immediately engaging them in practical, large-scale data analysis in real-time.
Participants will work hands-on from the very beginning, leveraging open physics data from sources like CERN, NASA, ESA, Zenodo, and arXiv.

Participants will learn how to access, analyse, and visualise large datasets using enterprise-grade and industrial tools such as GitHub, GitHub Actions, Jupyter Notebooks, and cloud-based platforms.
The workshop emphasises collaborative data analysis, workflow automation, and online publishing of reports.

Additionally, we will explore how to harness machine learning (ML) and large language models (LLMs) like LLaMA in Retrieval-Augmented Generation (RAG) workflows for literature review, merging multiple papers, and conducting meta-analyses.
A special focus will be on how researchers can use ML, AI, and RAG tools to write, read, and evaluate scientific papers, thereby enhancing their competitiveness on a global scale.

### **Learning Objectives:**

By the end of this workshop, participants will be able to:

- **Engage in Real-Time, Large-Scale Data Analysis:**
  - Perform hands-on data analysis of large datasets from the outset.
  - Utilise industrial tools to process and analyse data efficiently.

- **Leverage Enterprise-Grade Tools:**
  - Use Git and GitHub for version control and collaboration in research projects.
  - Automate data workflows and report generation using GitHub Actions.
  - Publish professional reports and interactive visualisations on GitHub Pages.

- **Access and Manipulate Open Datasets:**
  - Work with open datasets from CERN, NASA, ESA, Zenodo, and arXiv.
  - Perform particle physics and imaging data analysis using Jupyter Notebooks and Python libraries.

- **Apply Advanced Machine Learning Techniques:**
  - Utilise industrial and commercial-grade ML techniques to analyse datasets.
  - Implement ML models for classification, regression, and clustering tasks.

- **Utilize AI and LLMs in Research:**
  - Understand how ML, AI, and RAG tools are transforming the writing, reading, and evaluation of scientific papers.
  - Use LLMs in RAG workflows for literature review and meta-analysis.

- **Optimize Computational Resources:**
  - Learn how to access free and low-cost computational resources for scientific data collection, analysis, and reporting.
  - Set up personal data and computing environments using cloud platforms.

- **Enhance Global Competitiveness:**
  - Leverage tools and techniques to improve competitiveness as researchers in the global scientific community.
  - Apply best practices in open science, focusing on reproducibility, transparency, and ethical data sharing.

---

### **Prerequisites:**

- Basic understanding of programming.
- Familiarity with fundamental physics concepts and data interpretation.
- Personal laptop with internet access.
- Pre-installed software:
  - **Git**
  - **Anaconda** (for Jupyter Notebooks) or access to **Google Colab**.
- Accounts:
  - **GitHub** account (free to create).

---

### **Expected Outcomes:**

By the end of this workshop, participants will have:

- Hands-on experience with real-time, large-scale data analysis using industrial-grade tools.
- The ability to set up personal computing environments leveraging free and low-cost resources.
- Enhanced skills and understanding of machine learning and AI, as well as applying LLMs in research.
- A published interactive report or dashboard showcasing their analysis.
- Improved competitiveness as researchers in the global scientific community.
- A network of peers and resources to continue developing their skills.
- Familiarity with the resources and collaborative environment provided by [https://github.com/laa-hecap](https://github.com/laa-hecap).

---

**Key Takeaways:**

- **Immediate Engagement:** Participants will start working hands-on from the very first session, ensuring practical understanding and application.
- **Real-Time, Large-Scale Analysis:** The workshop focuses on processing and analysing substantial datasets, simulating real-world research scenarios.
- **Empowering Latin American Researchers:** By leveraging enterprise-grade tools and techniques, participants will enhance their global competitiveness.
- **Resource Optimization:** Learning to access and utilise **free** or low-cost computational resources will enable ongoing research without significant financial barriers.
- **Centralised Resources:** All necessary materials and references are conveniently located at [https://github.com/laa-hecap](https://github.com/laa-hecap), simplifying access and collaboration.

---

**Please note:** The GitHub organisation at [https://gitsub.com/laa-hecap](https://github.com/laa-hecap) is the only reference you will need for this workshop.
All materials, including code examples, datasets, tutorials, and additional resources, will be available there. Ensure you have access to this organisation before the workshop begins.

---

### **Instructor Information:**

- **Instructor Name**: Arturo SÃ¡nchez Pineda
- [https://www.linkedin.com/in/arturo-sanchez-pineda/](https://www.linkedin.com/in/arturo-sanchez-pineda/)
- **Affiliation**: Creative Commons Venezuela - Work and Lead IT Infra at [inait.ai](inait.ai)

---

### **Workshop Schedule:** **(UNDER CONSTRUCTION)**

#### *NSession 1:** Immediate Hands-On Introduction to Git, GitHub, and GitHub Pages
- **Date/Time:** [TBD]
- **Duration:** 2 hours

#### **Session 2:** Real-Time Data Analysis with Open Physics Data and Machine Learning Techniques
- **Date/Time:** [TBD]
- **Duration:** 2 hours

#### **Session 3:** Automating Workflows, Utilizing LLMs, and AI in Scientific Research
- **Date/Time:** [TBD]
- **Duration:** 2 hours

#### **Session 4:** Building Personal Data Centers, Advanced Visualization, Interactive Reporting, and RAG for Meta-Analysis
- **Date/Time:** [TBD]
- **Duration:** 2 hours

---

### **Detailed Session Outlines:**

#### **Session 1: Immediate Hands-On Introduction to Git, GitHub, and GitHub Pages**

**Emphasis:** Participants will start working hands-on from the beginning, setting up repositories and publishing content within the first session.

**Topics Covered:**

- **Quick Start with Git and GitHub (Hands-On):**
  - Immediate setup of Git on personal laptops.
  - Create a GitHub account and initialise a repository.
  - Cloning, committing, and pushing changes in real-time.

- **Collaborative Workflows:**
  - Working with branches and pull requests.
  - Collaborating with peers on shared repositories.

- **Setting Up GitHub Pages (Hands-On):**
  - Enabling GitHub Pages for the repository.
  - Publishing a live webpage/report immediately.
  - Customizing the site using themes and Markdown.

**Hands-on Activities:**

- **Repository Setup and Content Publishing:**
  - Participants create a repository and publish a GitHub Pages site within the first hour.
  - **Using Resources from [https://github.com/laa-hecap](https://github.com/laa-hecap):**
    - This GitHub organisation will provide all workshop materials, examples, and templates.
    - Participants will clone repositories from this organisation to get started.
  - Real-time collaboration with fellow participants on shared projects.

---

#### **Session 2: Real-Time Data Analysis with Open Physics Data and Machine Learning Techniques**

**Emphasis:** Engaging in large-scale data analysis in real-time, using industrial-grade tools to process substantial datasets.

**Topics Covered:**

- **Accessing and Processing Large Open Datasets (Hands-On):**
  - Downloading and handling large datasets from CERN, NASA, ESA, Zenodo, and arXiv.
  - Techniques for efficient data loading and preprocessing.

- **Immediate Data Analysis:**
  - Performing exploratory data analysis on large-scale datasets.
  - Visualizing data distributions and patterns.

- **Implementing Machine Learning Models (Hands-On):**
  - Apply industrial-grade ML techniques using libraries like TensorFlow or PyTorch.
  - Training and evaluating results in during the workshop.

**Hands-on Activities:**

- **Large-Scale Data Analysis:**
  - Participants work with substantial datasets, performing analysis and visualisation.
  - **Utilizisg Notebooks and Code from [https://github.com/laa-hecap](https://github.com/laa-hecap):**
    - Participants will use prepared notebooks and scripts from the organisation to facilitate analysis.
    - Modify and extend these resources to suit their research interests.
  - Implement ML models to derive insights from the data.

---

#### **Session 3: Automating Workflows, Utilizing LLMs, and AI in Scientific Research**

**Emphasis:** Leveraging automation and AI tools to enhance research efficiency and competitiveness.

**Topics Covered:**

- **Automating Data Workflows with GitHub Actions (Hands-On):**
  - Setting up workflows to automate data processing and analysis tasks.
  - Real-time deployment of automated pipelines.

- **Leveraging LLMs and AI Tools (Hands-On):**
  - Utilizing open-source LLMs like LLaMA for literature review.
  - Implementing RAG workflows to summarise and synthesise information from multiplespapers.

- **AI in Scientific Writing and Evaluation:**
  - Hands-on experience with AI tools for drafting and editing scientific papers.
  - Evaluating the impact and quality of research using AI-powered tools.

**Hands-on Activities:**

- **Workflow Automation:**
  - Participants create and deploy automated workflows that run data analysis and update reports.
  - **Reference Workflows from [https://github.com/laa-hecap](https://github.com/laa-hecap):**
    - Sample workflows and action scripts will be available for participants to use and adapt.

- **AI and LLM Implementation:**
  - Hands-on use of LLMs to conduct a literature review and generate a meta-analysis in real-time.
  - Resources a-d examples provided via the GitHub organization.

---

#### **Session 4: Building Personal Data Centers, Advanced Visualization, Interactive Reporting, and RAG for Meta-Analysis**

**Emphasis:** Empowering participants to set up personal computing environments and create interactive, professional reports.

**Topics Covered:**

- **Setting Up Personal Data and Computing Environments (Hands-On):**
  - Utilizing free and low-cost cloud resources.
  - Configuring environments for large-scale data processing.

- **Advanced Data Visualization (Hands-On):**
  - Creating interactive visualisations using enterprise-grade tools like Plotly and Bokeh.
  - Embedding interactive content into GitHub Pages.

- **Interactive Reporting and Dashboards:**
  - Building dashboards and deploying them online.
  - Real-time updates and interactivity in reports.

- **Advanced RAG Techniques for Meta-Analysis (Hands-On):**
  - Integrating multiple data sources into comprehensive analyses.
  - Using LLMs to produce cohesive reports from diverse datasets.

**Hands-on Activities:**

- **Building a Personal Data Center:**
  - Participants set up their own data processing environment using cloud services.
  - **Guides and Scripts from [https://github.com/laa-hecap](https://github.com/laa-hecap):**
    - Utilise provided resources to streamline setup.

- **Interactive Reporting:**
  - Creating and deploying an interactive dashboard that visualises analysis results in real-time.
  - Leverage examples and templates from the GitHub organisation.

---

### **Materials and Resources:**

**Primary Reference:**

- **GitHub Organization:** [https://github.com/laa-hecap](https://github.com/laa-hecap)
  - **All workshop materials, resources, code examples, datasets, and templates will be available through this GitHub organisation.**
  - **Participants will use this as their main and only reference throughout the workshop.**

**Software and Tools:**

- **Git**
- **Anaconda** or **Google Colab**
- **Python Libraries:**
  - NumPy, Pandas, Matplotlib, Seaborn, Plotly, scikit-learn, TensorFlow, PyTorch, Hugging Face Transformers

**Datasets and Publications:**

- Accessed via resources in [https://github.com/laa-hecap](https://github.com/laa-hecap)

---

### **Additional Notes:**

- **Preparation Before the Workshop:**
  - Install all required software and create necessary accounts.
  - **No need to ensure your laptop can handle and has a stable internet connection.**
  - Familiarise yourself with the GitHub organization [https:/sgithub.com/laa-hecap](https://github.com/laa-hecap).

- **During the Workshop:**
  - Be prepared to engage in hands-on activities immediately.
  - All necessary materials and references will be provided through the GitHub organisation.
  - Bring suggestions and be ready to collaborate with peers.

- **Post-Workshop Resources:**
  - Continued access to all materials, code samples, and datasets via [https://github.com/laa-hecap](https://github.com/laa-hecap).

---


